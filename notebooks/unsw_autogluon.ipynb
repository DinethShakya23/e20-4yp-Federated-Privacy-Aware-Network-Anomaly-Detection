{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45fa0908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251224_131436\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.10.19\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #89~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Wed Oct 29 10:45:25 UTC 2\n",
      "CPU Count:          32\n",
      "Pytorch Version:    2.9.1+cu128\n",
      "CUDA Version:       12.8\n",
      "GPU Memory:         GPU 0: 47.52/47.53 GB | GPU 1: 47.52/47.52 GB\n",
      "Total GPU Memory:   Free: 95.04 GB, Allocated: 0.01 GB, Total: 95.05 GB\n",
      "GPU Count:          2\n",
      "Memory Avail:       18.97 GB / 125.59 GB (15.1%)\n",
      "Disk Space Avail:   1279.91 GB / 11087.69 GB (11.5%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme'  : New in v1.5: The state-of-the-art for tabular data. Massively better than 'best' on datasets <100000 samples by using new Tabular Foundation Models (TFMs) meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, TabDPT, and TabM. Requires a GPU and `pip install autogluon.tabular[tabarena]` to install TabPFN, TabICL, and TabDPT.\n",
      "\tpresets='best'     : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='best_v150': New in v1.5: Better quality than 'best' and 5x+ faster to train. Give it a try!\n",
      "\tpresets='high'     : Strong accuracy with fast inference speed.\n",
      "\tpresets='high_v150': New in v1.5: Better quality than 'high' and 5x+ faster to train. Give it a try!\n",
      "\tpresets='good'     : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'   : Fast training time, ideal for initial prototyping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Analysis': np.float64(9.623622782446311), 'Backdoor': np.float64(11.06484165324745), 'DoS': np.float64(1.5757376547928452), 'Exploits': np.float64(0.5787142055025267), 'Fuzzers': np.float64(1.0627313502087952), 'Generic': np.float64(0.4376881754676519), 'Normal': np.float64(0.2770672043010753), 'Reconnaissance': np.float64(1.8423272857270534), 'Shellcode': np.float64(17.050289495450787), 'Worms': np.float64(148.30071942446042)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Values in column 'sample_weight' used as sample weights instead of predictive features. Evaluation metrics will ignore sample weights, specify weight_evaluation=True to instead report weighted metrics.\n",
      "Beginning AutoGluon training ... Time limit = 7200s\n",
      "AutoGluon will save models to \"/home/e20094/e20-4yp-backdoor-resilient-federated-nids/notebooks/AutogluonModels/ag-20251224_131436\"\n",
      "Train Data Rows:    206138\n",
      "Train Data Columns: 43\n",
      "Label Column:       attack_cat\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 10\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18387.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 96.59 MB (0.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 1): ['ct_ftp_cmd']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['ct_ftp_cmd']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 11 | ['dur', 'rate', 'sload', 'dload', 'sinpkt', ...]\n",
      "\t\t('int', [])    : 27 | ['spkts', 'dpkts', 'sbytes', 'dbytes', 'sttl', ...]\n",
      "\t\t('object', []) :  3 | ['proto', 'service', 'state']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  3 | ['proto', 'service', 'state']\n",
      "\t\t('float', [])     : 11 | ['dur', 'rate', 'sload', 'dload', 'sinpkt', ...]\n",
      "\t\t('int', [])       : 26 | ['spkts', 'dpkts', 'sbytes', 'dbytes', 'sttl', ...]\n",
      "\t\t('int', ['bool']) :  1 | ['is_sm_ips_ports']\n",
      "\t1.7s = Fit runtime\n",
      "\t41 features in original data used to generate 41 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 59.17 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{'num_epochs': 50, 'hidden_size': 256, 'dropout_prob': 0.3}, {'num_epochs': 80, 'hidden_size': 512, 'dropout_prob': 0.4}, {'num_epochs': 100, 'hidden_size': 1024, 'dropout_prob': 0.5}],\n",
      "}\n",
      "Fitting 3 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 7197.74s of the 7197.69s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=2.27%)\n",
      "\t0.8223\t = Validation score   (accuracy)\n",
      "\t1335.88s\t = Training   runtime\n",
      "\t5.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_2_BAG_L1 ... Training model for up to 5850.97s of the 5850.91s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.11%)\n",
      "\t0.8252\t = Validation score   (accuracy)\n",
      "\t2279.82s\t = Training   runtime\n",
      "\t7.73s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_3_BAG_L1 ... Training model for up to 3556.64s of the 3556.57s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (2.0 workers, per: cpus=1, gpus=1, memory=1.06%)\n",
      "\t0.8251\t = Validation score   (accuracy)\n",
      "\t2730.09s\t = Training   runtime\n",
      "\t6.36s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 719.77s of the 796.23s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=32, gpus=0, mem=0.1/24.5 GB\n",
      "\tEnsemble Weights: {'NeuralNetTorch_2_BAG_L1': 0.5, 'NeuralNetTorch_3_BAG_L1': 0.5}\n",
      "\t0.826\t = Validation score   (accuracy)\n",
      "\t5.24s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 6416.21s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 2922.8 rows/s (41228 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/e20094/e20-4yp-backdoor-resilient-federated-nids/notebooks/AutogluonModels/ag-20251224_131436\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "{'accuracy': 0.8250121276802174, 'balanced_accuracy': np.float64(0.5009610944711875), 'mcc': 0.7758936322853301}\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      Analysis       0.94      0.05      0.10       535\n",
      "      Backdoor       0.68      0.08      0.15       466\n",
      "           DoS       0.70      0.05      0.10      3271\n",
      "      Exploits       0.61      0.94      0.74      8905\n",
      "       Fuzzers       0.70      0.53      0.60      4849\n",
      "       Generic       0.99      0.98      0.99     11774\n",
      "        Normal       0.90      0.94      0.92     18600\n",
      "Reconnaissance       0.91      0.76      0.82      2798\n",
      "     Shellcode       0.57      0.56      0.57       302\n",
      "         Worms       0.80      0.11      0.20        35\n",
      "\n",
      "      accuracy                           0.83     51535\n",
      "     macro avg       0.78      0.50      0.52     51535\n",
      "  weighted avg       0.84      0.83      0.80     51535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "train_path = os.path.join(PROJECT_ROOT, \"data\", \"unsw-nb15\",\"raw\", \"UNSW_NB15_training-set.csv\")\n",
    "test_path = os.path.join(PROJECT_ROOT, \"data\", \"unsw-nb15\",\"raw\", \"UNSW_NB15_testing-set.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "DROP_COLS = [\"id\", \"label\"]\n",
    "train_df = train_df.drop(columns=DROP_COLS, errors=\"ignore\")\n",
    "test_df  = test_df.drop(columns=DROP_COLS, errors=\"ignore\")\n",
    "\n",
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    full_df,\n",
    "    test_size=0.2,\n",
    "    stratify=full_df[\"attack_cat\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "LABEL = \"attack_cat\"\n",
    "\n",
    "# Compute class weights\n",
    "classes = np.unique(train_df[LABEL])\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=train_df[LABEL]\n",
    ")\n",
    "class_weights = dict(zip(classes, weights))\n",
    "print(class_weights)\n",
    "\n",
    "# Create sample weights for training data only\n",
    "train_df['sample_weight'] = train_df[LABEL].map(class_weights)\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=\"attack_cat\",\n",
    "    problem_type=\"multiclass\",\n",
    "    eval_metric=\"accuracy\",\n",
    "    sample_weight='sample_weight'\n",
    ")\n",
    "\n",
    "# Force GPU usage with proper configuration\n",
    "predictor.fit(\n",
    "    train_data=train_df,\n",
    "    hyperparameters = {\n",
    "    \"NN_TORCH\": [\n",
    "        {\n",
    "            \"num_epochs\": 50,\n",
    "            \"hidden_size\": 256,\n",
    "            \"dropout_prob\": 0.3\n",
    "        },\n",
    "        {\n",
    "            \"num_epochs\": 80,\n",
    "            \"hidden_size\": 512,\n",
    "            \"dropout_prob\": 0.4\n",
    "        },\n",
    "        {\n",
    "            \"num_epochs\": 100,\n",
    "            \"hidden_size\": 1024,\n",
    "            \"dropout_prob\": 0.5\n",
    "        }\n",
    "    ]\n",
    "},\n",
    "    time_limit=7200,\n",
    "    num_stack_levels=0,  # Disable stacking completely for faster training\n",
    "    num_bag_folds=5,\n",
    "    ag_args_fit={\n",
    "        \"num_gpus\": 1  # Global GPU setting\n",
    "    }\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "results = predictor.evaluate(test_df)\n",
    "print(\"\\nTest Results:\")\n",
    "print(results)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = predictor.predict(test_df)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df[LABEL], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b893f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('category', [])  :  3 | ['proto', 'service', 'state']\n",
      "('float', [])     : 11 | ['dur', 'rate', 'sload', 'dload', 'sinpkt', ...]\n",
      "('int', [])       : 26 | ['spkts', 'dpkts', 'sbytes', 'dbytes', 'sttl', ...]\n",
      "('int', ['bool']) :  1 | ['is_sm_ips_ports']\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "predictor = TabularPredictor.load(\"/home/e20094/e20-4yp-backdoor-resilient-federated-nids/notebooks/AutogluonModels/ag-20251224_121820\")\n",
    "print(predictor.feature_metadata)\n",
    "# This will confirm exactly which columns were treated as what type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5d2787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1. Best Model Info ===\n",
      "Best Model: WeightedEnsemble_L2\n",
      "\n",
      "=== 2. Analyzing: NeuralNetTorch_BAG_L1 ===\n",
      "\n",
      "--- Hyperparameters ---\n",
      "{'bin': 'auto',\n",
      " 'max_base_models': 0,\n",
      " 'max_base_models_per_type': 'auto',\n",
      " 'model_random_seed': 0,\n",
      " 'n_bins': None,\n",
      " 'save_bag_folds': True,\n",
      " 'stratify': 'auto',\n",
      " 'use_orig_features': True,\n",
      " 'valid_stacker': True,\n",
      " 'vary_seed_across_folds': False}\n",
      "\n",
      "--- Model Arguments (Architecture) ---\n",
      "{'drop_unique': False,\n",
      " 'get_features_kwargs': None,\n",
      " 'get_features_kwargs_extra': None,\n",
      " 'ignored_type_group_raw': None,\n",
      " 'ignored_type_group_special': None,\n",
      " 'max_memory_usage_ratio': 1.0,\n",
      " 'max_time_limit': None,\n",
      " 'max_time_limit_ratio': 1.0,\n",
      " 'min_time_limit': 0,\n",
      " 'predict_1_batch_size': None,\n",
      " 'temperature_scalar': None,\n",
      " 'valid_raw_types': None,\n",
      " 'valid_special_types': None}\n",
      "\n",
      "=== 3. Feature Metadata ===\n",
      "('category', [])  :  3 | ['proto', 'service', 'state']\n",
      "('float', [])     : 11 | ['dur', 'rate', 'sload', 'dload', 'sinpkt', ...]\n",
      "('int', [])       : 26 | ['spkts', 'dpkts', 'sbytes', 'dbytes', 'sttl', ...]\n",
      "('int', ['bool']) :  1 | ['is_sm_ips_ports']\n",
      "\n",
      "=== 4. Exact Input Feature List ===\n",
      "Feature generator not directly accessible, relying on metadata above.\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import pprint\n",
    "\n",
    "# 1. Load the AutoGluon Predictor\n",
    "save_path = \"/home/e20094/e20-4yp-backdoor-resilient-federated-nids/notebooks/AutogluonModels/ag-20251224_121820\"\n",
    "predictor = TabularPredictor.load(save_path)\n",
    "\n",
    "print(\"=== 1. Best Model Info ===\")\n",
    "# FIX: Use .model_best (property) instead of .get_model_best()\n",
    "best_model_name = predictor.model_best\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "\n",
    "# 2. Inspect the Neural Network specifically\n",
    "model_names = predictor.model_names() # Updated method name\n",
    "nn_models = [m for m in model_names if \"NeuralNetTorch\" in m]\n",
    "\n",
    "if nn_models:\n",
    "    nn_name = nn_models[0]\n",
    "    print(f\"\\n=== 2. Analyzing: {nn_name} ===\")\n",
    "    \n",
    "    # Get model info / hyperparameters\n",
    "    model_info = predictor.model_info(nn_name)\n",
    "    \n",
    "    print(\"\\n--- Hyperparameters ---\")\n",
    "    # We specifically want to see hidden_size, activation, etc.\n",
    "    pprint.pprint(model_info.get('hyperparameters', {}))\n",
    "    \n",
    "    print(\"\\n--- Model Arguments (Architecture) ---\")\n",
    "    # Sometimes architecture details are in 'ag_args_fit' or root keys\n",
    "    pprint.pprint(model_info.get('ag_args_fit', {}))\n",
    "\n",
    "    print(\"\\n=== 3. Feature Metadata ===\")\n",
    "    # This tells us exactly which features are float vs int vs category\n",
    "    print(predictor.feature_metadata)\n",
    "    \n",
    "    print(\"\\n=== 4. Exact Input Feature List ===\")\n",
    "    # These are the columns expected by the model after dropping useless ones\n",
    "    # In 1.5.0 we can often access this via feature_generator\n",
    "    if hasattr(predictor, 'feature_generator'):\n",
    "        print(predictor.feature_generator.features_in)\n",
    "    else:\n",
    "        print(\"Feature generator not directly accessible, relying on metadata above.\")\n",
    "\n",
    "else:\n",
    "    print(\"No NeuralNetTorch model found in the list.\")\n",
    "    print(\"Available models:\", model_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-nids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
