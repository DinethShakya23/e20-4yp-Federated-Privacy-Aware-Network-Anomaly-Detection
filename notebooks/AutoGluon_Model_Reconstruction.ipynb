{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d57693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d36932",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "train_path = os.path.join(PROJECT_ROOT, \"data\", \"unsw-nb15\",\"raw\", \"UNSW_NB15_training-set.csv\")\n",
    "test_path = os.path.join(PROJECT_ROOT, \"data\", \"unsw-nb15\",\"raw\", \"UNSW_NB15_testing-set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b90d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "DROP_COLS = [\"id\", \"label\"]\n",
    "train_df = train_df.drop(columns=DROP_COLS, errors=\"ignore\")\n",
    "test_df  = test_df.drop(columns=DROP_COLS, errors=\"ignore\")\n",
    "\n",
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    full_df,\n",
    "    test_size=0.2,\n",
    "    stratify=full_df[\"attack_cat\"],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e0f3155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('category', [])  :  3 | ['proto', 'service', 'state']\n",
      "('float', [])     : 11 | ['dur', 'rate', 'sload', 'dload', 'sinpkt', ...]\n",
      "('int', [])       : 26 | ['spkts', 'dpkts', 'sbytes', 'dbytes', 'sttl', ...]\n",
      "('int', ['bool']) :  1 | ['is_sm_ips_ports']\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "predictor = TabularPredictor.load(\n",
    "    \"/home/e20094/e20-4yp-backdoor-resilient-federated-nids/notebooks/AutogluonModels/ag-20251224_121820\"\n",
    ")\n",
    "\n",
    "print(predictor.feature_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0678dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autogluon.core.models.ensemble.stacker_ensemble_model.StackerEnsembleModel'>\n",
      "<autogluon.core.models.ensemble.stacker_ensemble_model.StackerEnsembleModel object at 0x7b6ddb231d80>\n"
     ]
    }
   ],
   "source": [
    "model = predictor._trainer.load_model(\"NeuralNetTorch_BAG_L1\")\n",
    "print(type(model))\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fc0ebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child models:\n",
      "['S1F1', 'S1F2', 'S1F3', 'S1F4', 'S1F5']\n"
     ]
    }
   ],
   "source": [
    "bag = predictor._trainer.load_model(\"NeuralNetTorch_BAG_L1\")\n",
    "\n",
    "print(\"Child models:\")\n",
    "print(bag.models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e226ba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pkl\n"
     ]
    }
   ],
   "source": [
    "!ls AutogluonModels/ag-20251224_121820/models/NeuralNetTorch_BAG_L1/S1F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "03b81a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>\n",
      "<autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel object at 0x7b6c5230ba30>\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch import TabularNeuralNetTorchModel\n",
    "\n",
    "fold_path = \"/home/e20094/e20-4yp-backdoor-resilient-federated-nids/notebooks/AutogluonModels/ag-20251224_121820/models/NeuralNetTorch_BAG_L1/S1F1\"\n",
    "\n",
    "fold_model = TabularNeuralNetTorchModel.load(fold_path)\n",
    "\n",
    "print(type(fold_model))\n",
    "print(fold_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dbb08ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL KEYS:\n",
      "['model', 'features', 'feature_metadata', '_features_internal', '_features_internal_to_align', '_feature_metadata', '_is_features_in_same_as_ex', '_is_fit_metadata_registered', '_fit_metadata', '_types_of_features', 'feature_arraycol_map', 'feature_type_map', 'features_to_drop', 'processor', 'num_dataloading_workers']\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL KEYS:\")\n",
    "print([k for k in fold_model.__dict__.keys()\n",
    "       if \"data\" in k.lower()\n",
    "       or \"processor\" in k.lower()\n",
    "       or \"feature\" in k.lower()\n",
    "       or \"network\" in k.lower()\n",
    "       or \"model\" in k.lower()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29e5e78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'path_root', 'path', 'num_classes', 'quantile_levels', 'model', 'problem_type', 'conformalize', 'label_cleaner', 'eval_metric', 'stopping_metric', 'normalize_pred_probas', 'features', 'feature_metadata', '_features_internal', '_features_internal_to_align', '_feature_metadata', '_is_features_in_same_as_ex', 'fit_time', 'predict_time', '_predict_n_size', 'predict_1_time', 'compile_time', 'val_score', '_memory_usage_estimate', '_user_params', '_user_params_aux', 'params', 'params_aux', 'params_trained', 'nondefault_params', '_is_initialized', '_is_fit_metadata_registered', '_fit_metadata', 'saved_learning_curves', '_compiler', 'random_seed', '_types_of_features', 'feature_arraycol_map', 'feature_type_map', 'features_to_drop', 'processor', 'num_dataloading_workers', '_architecture_desc', 'optimizer', 'device', 'max_batch_size', '_num_cpus_infer']\n"
     ]
    }
   ],
   "source": [
    "print([k for k in fold_model.__dict__.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b00599ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.compose._column_transformer.ColumnTransformer'>\n",
      "ColumnTransformer(force_int_remainder_cols=False, remainder='passthrough',\n",
      "                  transformers=[('continuous',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='median')),\n",
      "                                                 ('scaler', StandardScaler())]),\n",
      "                                 ['sttl', 'dttl', 'swin', 'dwin']),\n",
      "                                ('skewed',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='median')),\n",
      "                                                 ('quantile',\n",
      "                                                  QuantileTransformer(output_distribution='nor...\n",
      "                                  'sinpkt', 'dinpkt', 'sjit', 'djit', 'stcpb',\n",
      "                                  'dtcpb', 'tcprtt', 'synack', 'ackdat',\n",
      "                                  'smean', 'dmean', 'trans_depth',\n",
      "                                  'response_body_len', 'ct_srv_src',\n",
      "                                  'ct_state_ttl', 'ct_dst_ltm',\n",
      "                                  'ct_src_dport_ltm', 'ct_dst_sport_ltm',\n",
      "                                  'ct_dst_src_ltm', 'is_ftp_login', ...]),\n",
      "                                ('ordinal',\n",
      "                                 Pipeline(steps=[('ordinal',\n",
      "                                                  OrdinalMergeRaresHandleUnknownEncoder(max_levels=100))]),\n",
      "                                 ['proto', 'service', 'state'])])\n"
     ]
    }
   ],
   "source": [
    "processor = fold_model.processor\n",
    "print(type(processor))\n",
    "print(processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "27f88a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor attributes:\n",
      "['transformers', 'remainder', 'sparse_threshold', 'n_jobs', 'transformer_weights', 'verbose', 'verbose_feature_names_out', 'force_int_remainder_cols', 'feature_names_in_', 'n_features_in_', '_columns', '_transformer_to_input_indices', '_remainder', 'sparse_output_', 'transformers_', 'output_indices_']\n"
     ]
    }
   ],
   "source": [
    "print(\"Processor attributes:\")\n",
    "print([k for k in processor.__dict__.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b74d7e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External features: ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports', 'proto', 'service', 'state']\n",
      "Internal features: ['dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth', 'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports', 'proto', 'service', 'state']\n",
      "Feature array col map: OrderedDict([('sttl', [0]), ('dttl', [1]), ('swin', [2]), ('dwin', [3]), ('dur', [4]), ('spkts', [5]), ('dpkts', [6]), ('sbytes', [7]), ('dbytes', [8]), ('rate', [9]), ('sload', [10]), ('dload', [11]), ('sloss', [12]), ('dloss', [13]), ('sinpkt', [14]), ('dinpkt', [15]), ('sjit', [16]), ('djit', [17]), ('stcpb', [18]), ('dtcpb', [19]), ('tcprtt', [20]), ('synack', [21]), ('ackdat', [22]), ('smean', [23]), ('dmean', [24]), ('trans_depth', [25]), ('response_body_len', [26]), ('ct_srv_src', [27]), ('ct_state_ttl', [28]), ('ct_dst_ltm', [29]), ('ct_src_dport_ltm', [30]), ('ct_dst_sport_ltm', [31]), ('ct_dst_src_ltm', [32]), ('is_ftp_login', [33]), ('ct_flw_http_mthd', [34]), ('ct_src_ltm', [35]), ('ct_srv_dst', [36]), ('proto', [37]), ('service', [38]), ('state', [39]), ('is_sm_ips_ports', [40])])\n",
      "Feature type map: OrderedDict([('sttl', 'vector'), ('dttl', 'vector'), ('swin', 'vector'), ('dwin', 'vector'), ('dur', 'vector'), ('spkts', 'vector'), ('dpkts', 'vector'), ('sbytes', 'vector'), ('dbytes', 'vector'), ('rate', 'vector'), ('sload', 'vector'), ('dload', 'vector'), ('sloss', 'vector'), ('dloss', 'vector'), ('sinpkt', 'vector'), ('dinpkt', 'vector'), ('sjit', 'vector'), ('djit', 'vector'), ('stcpb', 'vector'), ('dtcpb', 'vector'), ('tcprtt', 'vector'), ('synack', 'vector'), ('ackdat', 'vector'), ('smean', 'vector'), ('dmean', 'vector'), ('trans_depth', 'vector'), ('response_body_len', 'vector'), ('ct_srv_src', 'vector'), ('ct_state_ttl', 'vector'), ('ct_dst_ltm', 'vector'), ('ct_src_dport_ltm', 'vector'), ('ct_dst_sport_ltm', 'vector'), ('ct_dst_src_ltm', 'vector'), ('is_ftp_login', 'vector'), ('ct_flw_http_mthd', 'vector'), ('ct_src_ltm', 'vector'), ('ct_srv_dst', 'vector'), ('proto', 'embed'), ('service', 'embed'), ('state', 'embed'), ('is_sm_ips_ports', 'vector')])\n",
      "Features dropped: []\n"
     ]
    }
   ],
   "source": [
    "print(\"External features:\", fold_model.features)\n",
    "print(\"Internal features:\", fold_model._features_internal)\n",
    "print(\"Feature array col map:\", fold_model.feature_arraycol_map)\n",
    "print(\"Feature type map:\", fold_model.feature_type_map)\n",
    "print(\"Features dropped:\", fold_model.features_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "496e4462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbedNet(\n",
      "  (embed_blocks): ModuleList(\n",
      "    (0): Embedding(102, 21)\n",
      "    (1): Embedding(14, 7)\n",
      "    (2): Embedding(9, 5)\n",
      "  )\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=71, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = fold_model.model\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75f824dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['embed_blocks.0.weight', 'embed_blocks.1.weight', 'embed_blocks.2.weight', 'main_block.0.weight', 'main_block.0.bias', 'main_block.3.weight', 'main_block.3.bias', 'main_block.6.weight', 'main_block.6.bias', 'main_block.9.weight', 'main_block.9.bias', 'main_block.11.weight', 'main_block.11.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = net.state_dict()\n",
    "print(state_dict.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093cb3e4",
   "metadata": {},
   "source": [
    "## Replicating preprocessing and the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d95a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        # Continuous features\n",
    "        self.cont_features = ['sttl', 'dttl', 'swin', 'dwin']\n",
    "        self.cont_imputer = SimpleImputer(strategy='median')\n",
    "        self.cont_scaler = StandardScaler()\n",
    "        \n",
    "        # Skewed numeric features\n",
    "        self.skew_features = [\n",
    "            'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sload', 'dload',\n",
    "            'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'stcpb', 'dtcpb',\n",
    "            'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
    "            'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
    "            'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login',\n",
    "            'ct_flw_http_mthd', 'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports'\n",
    "        ]\n",
    "        self.skew_imputer = SimpleImputer(strategy='median')\n",
    "        self.skew_scaler = QuantileTransformer(output_distribution='normal', random_state=42)\n",
    "        \n",
    "        # Categorical features\n",
    "        self.cat_features = ['proto', 'service', 'state']\n",
    "        self.cat_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        \n",
    "        # Maintain feature order\n",
    "        self.feature_order = [\n",
    "            'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload',\n",
    "            'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb',\n",
    "            'dtcpb', 'dwin', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
    "            'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm',\n",
    "            'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
    "            'ct_srv_dst', 'is_sm_ips_ports', 'proto', 'service', 'state'\n",
    "        ]\n",
    "        \n",
    "    def fit(self, df):\n",
    "        self.cont_imputer.fit(df[self.cont_features])\n",
    "        self.cont_scaler.fit(self.cont_imputer.transform(df[self.cont_features]))\n",
    "        \n",
    "        self.skew_imputer.fit(df[self.skew_features])\n",
    "        self.skew_scaler.fit(self.skew_imputer.transform(df[self.skew_features]))\n",
    "        \n",
    "        self.cat_encoder.fit(df[self.cat_features])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        cont = self.cont_scaler.transform(self.cont_imputer.transform(df[self.cont_features]))\n",
    "        skew = self.skew_scaler.transform(self.skew_imputer.transform(df[self.skew_features]))\n",
    "        cat = self.cat_encoder.transform(df[self.cat_features])\n",
    "        # Concatenate in the AutoGluon internal feature order\n",
    "        X_dict = {f: [] for f in self.feature_order}\n",
    "        for f in self.feature_order:\n",
    "            if f in self.cont_features:\n",
    "                X_dict[f] = cont[:, self.cont_features.index(f)]\n",
    "            elif f in self.skew_features:\n",
    "                X_dict[f] = skew[:, self.skew_features.index(f)]\n",
    "            elif f in self.cat_features:\n",
    "                X_dict[f] = cat[:, self.cat_features.index(f)]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown feature: {f}\")\n",
    "        X_ordered = np.column_stack([X_dict[f] for f in self.feature_order])\n",
    "        return X_ordered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0496cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EmbedNet(nn.Module):\n",
    "    def __init__(self, embedding_sizes, num_continuous, hidden_size=256, num_classes=10, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Embeddings for categorical features\n",
    "        self.embed_blocks = nn.ModuleList([nn.Embedding(cat_size, emb_dim) \n",
    "                                           for cat_size, emb_dim in embedding_sizes])\n",
    "        \n",
    "        total_emb_dim = sum([emb_dim for _, emb_dim in embedding_sizes])\n",
    "        self.num_inputs = num_continuous + total_emb_dim\n",
    "        \n",
    "        # Main block\n",
    "        self.main_block = nn.Sequential(\n",
    "            nn.Linear(self.num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x_cont, x_cat):\n",
    "        # x_cat is integer encoded\n",
    "        x_emb = [emb_layer(x_cat[:, i].long()) for i, emb_layer in enumerate(self.embed_blocks)]\n",
    "        x_emb = torch.cat(x_emb, dim=1)\n",
    "        x = torch.cat([x_cont, x_emb], dim=1)\n",
    "        out = self.main_block(x)\n",
    "        out = self.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f67e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_sizes = [(102,21), (14,7), (9,5)]  # proto, service, state\n",
    "num_continuous = 38  # 41 features total - 3 categorical\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cea4d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate\n",
    "model = EmbedNet(embedding_sizes, num_continuous=num_continuous, num_classes=num_classes)\n",
    "\n",
    "# Example tensors\n",
    "x_cont = torch.randn(5, num_continuous)      # batch of 5\n",
    "x_cat = torch.zeros(5,3, dtype=torch.long)\n",
    "x_cat[:,0] = torch.randint(0, 102, (5,))  # proto\n",
    "x_cat[:,1] = torch.randint(0, 14, (5,))   # service\n",
    "x_cat[:,2] = torch.randint(0, 9, (5,))    # state\n",
    "\n",
    "\n",
    "y_pred = model(x_cont, x_cat)\n",
    "print(y_pred.shape)  # should be (5,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3df4017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S1F1', 'S1F2', 'S1F3', 'S1F4', 'S1F5']\n",
      "odict_keys(['embed_blocks.0.weight', 'embed_blocks.1.weight', 'embed_blocks.2.weight', 'main_block.0.weight', 'main_block.0.bias', 'main_block.3.weight', 'main_block.3.bias', 'main_block.6.weight', 'main_block.6.bias', 'main_block.9.weight', 'main_block.9.bias', 'main_block.11.weight', 'main_block.11.bias'])\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Load the entire predictor\n",
    "predictor_path = \"/home/e20094/e20-4yp-backdoor-resilient-federated-nids/notebooks/AutogluonModels/ag-20251224_121820\"\n",
    "predictor = TabularPredictor.load(predictor_path)\n",
    "\n",
    "# Get the bagged neural network\n",
    "bag_model = predictor._trainer.load_model('NeuralNetTorch_BAG_L1')\n",
    "\n",
    "# List child models\n",
    "print(bag_model.models)  # ['S1F1', 'S1F2', ...]\n",
    "\n",
    "# Load one child\n",
    "child_model = bag_model.load_child('S1F1')\n",
    "\n",
    "# Access PyTorch model\n",
    "torch_model = child_model.model\n",
    "state_dict = torch_model.state_dict()\n",
    "print(state_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2044eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dict keys: odict_keys(['embed_blocks.0.weight', 'embed_blocks.1.weight', 'embed_blocks.2.weight', 'main_block.0.weight', 'main_block.0.bias', 'main_block.3.weight', 'main_block.3.bias', 'main_block.6.weight', 'main_block.6.bias', 'main_block.9.weight', 'main_block.9.bias', 'main_block.11.weight', 'main_block.11.bias'])\n",
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# =========================\n",
    "# 1. Load AutoGluon Predictor and Extract Weights\n",
    "# =========================\n",
    "predictor_path = \"/home/e20094/e20-4yp-backdoor-resilient-federated-nids/notebooks/AutogluonModels/ag-20251224_121820\"\n",
    "predictor = TabularPredictor.load(predictor_path)\n",
    "\n",
    "# Load bagged NN\n",
    "bag_model = predictor._trainer.load_model('NeuralNetTorch_BAG_L1')\n",
    "child_model = bag_model.load_child('S1F1')\n",
    "\n",
    "# Extract PyTorch model and state_dict\n",
    "torch_model = child_model.model\n",
    "state_dict = torch_model.state_dict()\n",
    "print(\"State dict keys:\", state_dict.keys())\n",
    "\n",
    "# =========================\n",
    "# 2. Preprocessing\n",
    "# =========================\n",
    "# Continuous features (from your previous info)\n",
    "continuous_features = ['sttl', 'dttl', 'swin', 'dwin', 'dur', 'spkts', 'dpkts', 'sbytes', 'dbytes',\n",
    "                       'rate', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit',\n",
    "                       'stcpb', 'dtcpb', 'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
    "                       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm', 'ct_src_dport_ltm',\n",
    "                       'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'is_ftp_login', 'ct_flw_http_mthd',\n",
    "                       'ct_src_ltm', 'ct_srv_dst', 'is_sm_ips_ports']\n",
    "\n",
    "# Categorical features\n",
    "categorical_features = ['proto', 'service', 'state']\n",
    "\n",
    "# Load training data to fit scalers/encoders\n",
    "train_path = \"/home/e20094/e20-4yp-backdoor-resilient-federated-nids/data/unsw-nb15/raw/UNSW_NB15_training-set.csv\"\n",
    "train_df = pd.read_csv(train_path).drop(columns=[\"id\", \"label\"], errors=\"ignore\")\n",
    "\n",
    "# Continuous preprocessing\n",
    "cont_imputer = SimpleImputer(strategy=\"median\")\n",
    "cont_scaler = StandardScaler()\n",
    "train_cont = cont_scaler.fit_transform(cont_imputer.fit_transform(train_df[continuous_features]))\n",
    "\n",
    "# Categorical preprocessing\n",
    "cat_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "train_cat = cat_encoder.fit_transform(train_df[categorical_features])\n",
    "\n",
    "# =========================\n",
    "# 3. Define EmbedNet (replicating AutoGluon NN)\n",
    "# =========================\n",
    "class EmbedNet(nn.Module):\n",
    "    def __init__(self, embed_input_sizes, continuous_input_size, hidden_size=256, output_size=10, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Embeddings\n",
    "        self.embed_blocks = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, dim)\n",
    "            for num_categories, dim in embed_input_sizes\n",
    "        ])\n",
    "        \n",
    "        # Main fully connected block\n",
    "        total_embed_size = sum([dim for _, dim in embed_input_sizes])\n",
    "        self.main_block = nn.Sequential(\n",
    "            nn.Linear(continuous_input_size + total_embed_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x_cont, x_cat):\n",
    "        x_emb = [emb_layer(x_cat[:, i].long()) for i, emb_layer in enumerate(self.embed_blocks)]\n",
    "        x_emb = torch.cat(x_emb, dim=1)\n",
    "        x = torch.cat([x_cont, x_emb], dim=1)\n",
    "        x = self.main_block(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# =========================\n",
    "# 4. Instantiate and Load Weights\n",
    "# =========================\n",
    "# Define embedding sizes (from AutoGluon processor info)\n",
    "embed_input_sizes = [(102,21), (14,7), (9,5)]  # proto, service, state\n",
    "continuous_input_size = len(continuous_features)\n",
    "output_size = 10  # 10 classes\n",
    "\n",
    "model = EmbedNet(embed_input_sizes, continuous_input_size, hidden_size=256, output_size=output_size, dropout=0.1)\n",
    "model.load_state_dict(state_dict)  # load weights from S1F1\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# 5. Predict on new data\n",
    "# =========================\n",
    "# Example batch (replace with real preprocessed data)\n",
    "x_cont = torch.tensor(train_cont[:5], dtype=torch.float32)  # batch of 5\n",
    "# Ensure x_cat values are in valid range for embeddings\n",
    "x_cat_test = torch.zeros((5,3), dtype=torch.long)\n",
    "x_cat_test[:,0] = torch.randint(0,102,(5,))  # proto\n",
    "x_cat_test[:,1] = torch.randint(0,14,(5,))   # service\n",
    "x_cat_test[:,2] = torch.randint(0,9,(5,))    # state\n",
    "\n",
    "y_pred = model(x_cont, x_cat_test)\n",
    "print(y_pred.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl-nids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
